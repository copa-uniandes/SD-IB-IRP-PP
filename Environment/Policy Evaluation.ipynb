{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanbeta/My Drive/Investigaci√≥n/SCA/SD-IB-IRP-PP/Environment/InstanceGenerator.py:56: DeprecationWarning: invalid escape sequence '\\i'\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "from SD_IB_IRP_PPenv import steroid_IRP\n",
    "from Policies import policies\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "This Notebook has a complete Policy Evaluation function for the Stochastic-Dynamic Inventory-Routing-Problem with Perishable Products. First, the main parameters of the problem and the environment must be set. All the main customizable parameters are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################   Environment's parameters   #################################\n",
    "# Random seed\n",
    "rd_seed = 0\n",
    "\n",
    "# SD-IB-IRP-PP model's parameters\n",
    "backorders = 'backorders'\n",
    "stochastic_parameters = ['q','d']\n",
    "\n",
    "# Feature's parameters\n",
    "look_ahead = ['q','d']\n",
    "historical_data = ['*']\n",
    "\n",
    "# Action's parameters\n",
    "validate_action = True\n",
    "warnings = False\n",
    "\n",
    "# Other parameters\n",
    "num_episodes = 1\n",
    "env_config = { 'M': 10, 'K': 10, 'T': 7,  'F': 4, \n",
    "               'S': 4,  'LA_horizon': 3}\n",
    "\n",
    "q_params = {'distribution': 'c_uniform', 'min': 6, 'max': 20}\n",
    "d_params = {'distribution': 'log-normal', 'mean': 2, 'stdev': 0.5}\n",
    "\n",
    "p_params = {'distribution': 'd_uniform', 'min': 20, 'max': 60}\n",
    "h_params = {'distribution': 'd_uniform', 'min': 20, 'max': 60}\n",
    "\n",
    "#################################   Environment's parameters   #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the policy evaluation is defined. This function only takes one parameter, the number of episodes that will be runned of the environment. All the policies to be evaluated must be in the 'Policies.py' file. Any policy used must be able to receive the state, the additional information in _ and the environment as a parameters. Also, the policy function must return an action in the format defined in the Toying.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "def Policy_evaluation(num_episodes = 1000):\n",
    "    \n",
    "    rewards = {}\n",
    "    states = {}\n",
    "    real_actions = {}\n",
    "    backorders = {}\n",
    "    la_decisions = {}\n",
    "    tws = {}\n",
    "    env = steroid_IRP( look_ahead = look_ahead, \n",
    "                       historical_data = historical_data, \n",
    "                       backorders = backorders,\n",
    "                       stochastic_parameters = stochastic_parameters, \n",
    "                       env_config = env_config)\n",
    "\n",
    "    policy = policies()\n",
    "\n",
    "    for episode in range(2):\n",
    "\n",
    "        state, _ = env.reset(return_state = True, rd_seed = rd_seed, \n",
    "          q_params = q_params, \n",
    "          p_params = p_params,\n",
    "          d_params = d_params,\n",
    "          h_params = h_params)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            states[episode,env.t] = state\n",
    "            action, la_dec = policy.stochastic_rolling_horizon(state, _, env)\n",
    "            print(state)\n",
    "            print(action)\n",
    "            state, reward, done, real_action, _,  = env.step(action, validate_action = validate_action, warnings = warnings)\n",
    "\n",
    "            real_actions[episode,env.t] = real_action\n",
    "            backorders[episode,env.t] = _[\"backorders\"]\n",
    "            rewards[episode,env.t] = reward\n",
    "            la_decisions[episode,env.t] = la_dec\n",
    "            tws[episode,env.t] = _[\"sample_path_window_size\"]\n",
    "            \n",
    "    iterables = (env.Suppliers,env.Products,env.Samples,env.M_kt,env.O_k,env.Horizon)\n",
    "    costs = (env.c, env.h_t, env.p_t)\n",
    "\n",
    "    return rewards, states, real_actions, backorders, la_decisions, tws, iterables, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, states, real_actions, backorders, la_decisions, tws, iterables, costs = Policy_evaluation(num_episodes = num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_decisions(episode, states, real_actions, backorders, K, T):\n",
    "    # Initial Inventory level for product k, aged o\n",
    "    ii_0 = {t:states[episode,t] for t in T}\n",
    "    # Routing Decisions\n",
    "    rout = {t:real_actions[episode,t][0] for t in T}\n",
    "    # Purchasing decisions\n",
    "    purch = {t:real_actions[episode,t][1] for t in T}\n",
    "    # Backorders\n",
    "    back = {t:{k:backorders[episode,t][k] if k in backorders[episode,t] else 0 for k in K} for t in T}\n",
    "\n",
    "    return ii_0, rout, purch, back\n",
    "\n",
    "def get_lookahead_decisions(episode, day, la_decisions, T):\n",
    "    # Initial Inventory level\n",
    "    ii_0_la = {(t):la_decisions[episode,day][0][t-day] for t in T}\n",
    "    # Purchase Decisions\n",
    "    purch_la = {(t):la_decisions[episode,day][1][t-day] for t in T}\n",
    "    # Backorders\n",
    "    back_la = {(t):la_decisions[episode,day][2][t-day] for t in T}\n",
    "\n",
    "    return ii_0_la, purch_la, back_la\n",
    "\n",
    "def charts_max_axis_values(episode, states, real_actions, backorders, la_decisions, tws, iterables, costs):\n",
    "    \n",
    "    M, K, S, M_kt, O_k, T = iterables\n",
    "\n",
    "    ''' Realized, Historical Decisions '''\n",
    "    ii_0, rout, purch, back = get_historical_decisions(episode,states,real_actions,backorders, K, T)\n",
    "\n",
    "    ''' Lookahead Decisions '''\n",
    "    ii_0_la, purch_la, back_la = {}, {}, {}\n",
    "    for t in T:\n",
    "        ii_0_la[t], purch_la[t], back_la[t] = get_lookahead_decisions(episode, t, la_decisions, range(t,t+tws[episode,t]))\n",
    "\n",
    "    ''' First chart'''\n",
    "    initi = [sum(ii_0[t][k,o] for k in K for o in range(1,O_k[k]+1)) for t in T]\n",
    "    repl = [sum(purch[t][i,k] for i in M for k in K) for t in T]\n",
    "    backo = [sum(back[t][k] for k in K) for t in T]\n",
    "    ub1 = max([initi[t]+repl[t]+backo[t]] for t in T)\n",
    "\n",
    "    initi = {(tau,t,s):sum(ii_0_la[tau][t][s][k,o] for k in K for o in range(1,O_k[k]+1)) for tau in T for t in range(tau,tau+ tws[episode,tau]) for s in S}\n",
    "    repl = {(tau,t,s):sum(purch_la[tau][t][s][i,k] for i in M for k in K) for tau in T for t in range(tau,tau+ tws[episode,tau]) for s in S}\n",
    "    backo = {(tau,t,s):sum(back_la[tau][t][s][k] for k in K) for tau in T for t in range(tau,tau+ tws[episode,tau]) for s in S}\n",
    "\n",
    "    ub1 = max(ub1,*[initi[tau,t,s]+repl[tau,t,s] for tau in T for t in range(tau+1,tau+ tws[episode,tau]) for s in S])\n",
    "    ub1 = max(ub1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    initi = {(s,tau,t):sum(I0[tau][k,t,o,s] for k in K for o in range(1,O_k[k]+1)) for s in Sam for tau in T for t in TW[tau]}\n",
    "    repl = {(s,tau,t):sum(z[tau][\"g\"][i,k,t,s] for i in M for k in K) for s in Sam for tau in T for t in TW[tau]}\n",
    "    ub1 = max([initi[s,tau,t]+repl[s,tau,t] for s in Sam for tau in T for t in TW[tau]])\n",
    "\n",
    "    backo = {(s,tau,t):sum(b[tau][k,t,s] for k in K) for s in Sam for tau in T for t in TW[tau]}\n",
    "    ub1 = max(ub1,*[backo[s,tau,t] for s in Sam for tau in T for t in TW[tau]])\n",
    "\n",
    "    backo = {(s,tau):sum(b[tau][k,0,s] for k in K) for s in Sam for tau in T}\n",
    "    ub1 = max(ub1,*[initi[s,tau,0]+repl[s,tau,0]+backo[s,tau] for s in Sam for tau in T])\n",
    "    \n",
    "    initi = {tau:sum(I0[tau][k,0,o,0] for k in K for o in range(1,O_k[k]+1)) for tau in T}\n",
    "    repl = {(tau,alg):sum(z_real[tau][alg][i,k] for i in M for k in K) for tau in T for alg in [\"g\",\"mq\",\"r\"]}\n",
    "    backo = {(tau,alg):sum(b_real[tau][alg][k] for k in K) for tau in T for alg in [\"g\",\"mq\",\"r\"]}\n",
    "    ub1 = max(ub1,*[initi[tau]+repl[tau,alg]+backo[tau,alg] for tau in T for alg in [\"g\",\"mq\",\"r\"]])  \n",
    "\n",
    "    ub1 = (int(ub1/100)+2)*100\n",
    "\n",
    "\n",
    "def visualize_by_day(episode, day, states, real_actions, backorders, la_decisions, tws, iterables, costs, conf_level):\n",
    "    \n",
    "    azul = (41/255,122/255,204/255)\n",
    "    verde = (70/255,145/255,57/255)\n",
    "    naranja = (235/255,140/255,68/255)\n",
    "    morado = (99/255,45/255,235/255)\n",
    "    rosado = (199/255,93/255,169/255)\n",
    "\n",
    "    colors = {\"hold\":[naranja,\"palegreen\"],\n",
    "                   \"back\":[azul,\"cyan\"],\n",
    "                   \"purch\":[verde,\"palegreen\"],\n",
    "                   \"rout\":[rosado,\"violet\"],\n",
    "                   \"dem\":[morado,\"mediumpurple\"],\n",
    "                   \"box_avail\":[(91/255,179/255,77/255),(213/255,230/255,123/255)],\n",
    "                   \"box_prices\":[(181/255,21/255,0/255),(223/255,159/255,156/255)]}\n",
    "\n",
    "    M, K, S, M_kt, O_k, T = iterables\n",
    "\n",
    "    ''' Realized, Historical Decisions '''\n",
    "    hist_T = range(day+1)\n",
    "    ii_0, rout, purch, back = get_historical_decisions(episode,states,real_actions,backorders, K, hist_T)\n",
    "\n",
    "    ''' Look-ahead Decisions '''\n",
    "    la_T = range(day,day+tws[episode,day])\n",
    "    ii_0_la, purch_la, back_la = get_lookahead_decisions(episode, day, la_decisions, la_T)\n",
    "\n",
    "    spec = {\"height_ratios\":[1, 1, 1, 1],\"hspace\":0.25,\"bottom\":0.1,\"top\":0.9}\n",
    "    fig, (ax1,ax2,ax3,ax4) = plt.subplots(nrows=4,ncols=1,figsize=(13,20),gridspec_kw=spec)\n",
    "\n",
    "    ''' First Chart: Quantities '''\n",
    "    # Max value for the axis\n",
    "    ub1 = 100\n",
    "    # Realized decisions\n",
    "    for t in hist_T:\n",
    "        initi = sum(ii_0[t][k,o] for k in K for o in range(1,O_k[k]+1))\n",
    "        repl = sum(purch[t][i,k] for i in M for k in K)\n",
    "        backo = sum(back[t][k] for k in K)\n",
    "        if t == day:\n",
    "            x_adj = 0.2\n",
    "            wid = 0.4\n",
    "        else:\n",
    "            x_adj = 0\n",
    "            wid = 0.8\n",
    "        ax1.bar(x=t-x_adj, height=initi, color=colors[\"hold\"][0],width=wid)\n",
    "        ax1.bar(x=t-x_adj, height=repl, bottom=initi, color=colors[\"purch\"][0],width=wid)\n",
    "        ax1.bar(x=t-x_adj, height=backo, bottom=initi+repl, color=colors[\"back\"][0], width=wid)\n",
    "\n",
    "    # Look-ahead decisions\n",
    "    for t in la_T:\n",
    "        initi = [sum(ii_0_la[t][s][k,o] for k in K for o in range(1,O_k[k]+1)) for s in S]\n",
    "        repl = [sum(purch_la[t][s][i,k] for i in M for k in K) for s in S]\n",
    "        backo = [sum(back_la[t][s][k] for k in K) for s in S]\n",
    "        if t == day:\n",
    "            tq = [initi[s]+repl[s]+backo[s] for s in S]\n",
    "            cin = st.t.interval(alpha=conf_level, df=len(tq)-1, loc=sum(tq)/len(tq), scale=st.sem(tq)) \n",
    "            ax1.axvline(x=t+0.2,ymin=cin[0]/ub1,ymax=cin[1]/ub1,color=\"black\",marker=\"_\",mew=1.5,ms=8)\n",
    "            bot_back = (sum(initi)+sum(repl))/len(S)\n",
    "            x_adj = 0.2\n",
    "        else:\n",
    "            tq = [initi[s]+repl[s] for s in S]\n",
    "            cin = st.t.interval(alpha=conf_level, df=len(tq)-1, loc=sum(tq)/len(tq), scale=st.sem(tq)) \n",
    "            ax1.axvline(x=t-0.2,ymin=cin[0]/ub1,ymax=cin[1]/ub1,color=\"black\",marker=\"_\",mew=1.5,ms=8)\n",
    "            cin = st.t.interval(alpha=conf_level, df=len(backo)-1, loc=sum(backo)/len(backo), scale=st.sem(backo))\n",
    "            ax1.axvline(x=t+0.2,ymin=cin[0]/ub1,ymax=cin[1]/ub1,color=\"black\",marker=\"_\",mew=1.5,ms=8)\n",
    "            bot_back = 0\n",
    "            x_adj = -0.2\n",
    "        ax1.bar(x=t+x_adj, height=sum(initi)/len(S), color=colors[\"hold\"][0], width=0.4, alpha=0.5)\n",
    "        ax1.bar(x=t+x_adj, height=sum(repl)/len(S), bottom=sum(initi)/len(S), color=colors[\"purch\"][0], width=0.4, alpha = 0.5)\n",
    "        ax1.bar(x=t+0.2, height=sum(backo)/len(S), bottom=bot_back, color=colors[\"back\"][0], width=0.4, alpha=0.5)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92027e2c1da3bcebf34b84774ed38599ef17aa35363e249514df9920c133b09b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
